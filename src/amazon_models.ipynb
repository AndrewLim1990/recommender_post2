{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog post, I will explore the use of a collaborative filtering algorithm on a real movie dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description:\n",
    "\n",
    "As discussed in my last post, we can make a recommender system using a collaborative filtering algorithm. This algorithm will use data obtained from [here](http://jmcauley.ucsd.edu/data/amazon/). Big thanks to Julian for giving me access to this data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization:\n",
    "\n",
    "First, we will need to load appropriate libraries and do a little light wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing\n",
    "import feather\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from keras import backend as k\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants:\n",
    "#NUM_OF_REVIEWS = 1000000\n",
    "MIN_NUM_REVIEWS_MOVIE = 250\n",
    "MIN_NUM_REVIEWS_PERSON = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading data\n",
    "df = feather.read_dataframe(\"../results/movie_df.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrangling done below will only get movies that have reviewed a minimum amount of times. Additionally, only users that have rated a minimum amount of movies will be selected. This was mostly done because if I hadn't, I would often get a lot of obscure movie titles that I haven't heard of before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>review_time</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1393425</th>\n",
       "      <td>A328S9RN3U5M68</td>\n",
       "      <td>B00005JMFQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1068681600</td>\n",
       "      <td>Love Actually (Widescreen Edition)</td>\n",
       "      <td>13.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288400</th>\n",
       "      <td>A1NSHD4YCL5DV3</td>\n",
       "      <td>0792833236</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1063497600</td>\n",
       "      <td>Raging Bull</td>\n",
       "      <td>12.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913721</th>\n",
       "      <td>A2R9J5LULVKF6T</td>\n",
       "      <td>6304176287</td>\n",
       "      <td>4.0</td>\n",
       "      <td>998956800</td>\n",
       "      <td>Willy Wonka &amp;amp; Chocolate Factory [VHS]</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860416</th>\n",
       "      <td>A335GUD1YBS31U</td>\n",
       "      <td>6303617719</td>\n",
       "      <td>5.0</td>\n",
       "      <td>998438400</td>\n",
       "      <td>French Kiss [VHS]</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512736</th>\n",
       "      <td>A1TFH5Y9I9M3YN</td>\n",
       "      <td>B00005QCYC</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1005091200</td>\n",
       "      <td>Jurassic Park Trilogy</td>\n",
       "      <td>36.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewer_id        asin  overall  review_time  \\\n",
       "1393425  A328S9RN3U5M68  B00005JMFQ      5.0   1068681600   \n",
       "288400   A1NSHD4YCL5DV3  0792833236      4.0   1063497600   \n",
       "913721   A2R9J5LULVKF6T  6304176287      4.0    998956800   \n",
       "860416   A335GUD1YBS31U  6303617719      5.0    998438400   \n",
       "1512736  A1TFH5Y9I9M3YN  B00005QCYC      4.0   1005091200   \n",
       "\n",
       "                                             title  price  \n",
       "1393425         Love Actually (Widescreen Edition)  13.70  \n",
       "288400                                 Raging Bull  12.70  \n",
       "913721   Willy Wonka &amp; Chocolate Factory [VHS]   3.99  \n",
       "860416                           French Kiss [VHS]   0.99  \n",
       "1512736                      Jurassic Park Trilogy  36.27  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find number of reviewers per movie and \n",
    "# the number of movies reviewed by each person\n",
    "n_reviews_movie_df = df.groupby('asin').reviewer_id.nunique()\n",
    "n_reviews_person_df = df.groupby('reviewer_id').asin.nunique()\n",
    "\n",
    "# Get list of movies that have been reviewed\n",
    "# more than MIN_NUM_REVIEWS_MOVIE times and \n",
    "# users that have reviewed more than \n",
    "# MIN_NUM_REVIEWS_PERSON\n",
    "popular_movies = list(n_reviews_movie_df[n_reviews_movie_df > MIN_NUM_REVIEWS_MOVIE].index)\n",
    "critical_people = list(n_reviews_person_df[n_reviews_person_df > MIN_NUM_REVIEWS_PERSON].index)\n",
    "\n",
    "# Filter dataframe to use only popular movies\n",
    "# and critical people:\n",
    "popular_df = df[df['asin'].isin(popular_movies)]\n",
    "popular_df = popular_df[popular_df['reviewer_id'].isin(critical_people)]\n",
    "\n",
    "# Shuffle data:\n",
    "popular_df = shuffle(popular_df, random_state=42)\n",
    "\n",
    "# Peak at data:\n",
    "popular_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We now have a nice little dataframe. It's nice to see that VHS seems to be alive and kicking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>review_time</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1393425</th>\n",
       "      <td>A328S9RN3U5M68</td>\n",
       "      <td>B00005JMFQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1068681600</td>\n",
       "      <td>Love Actually (Widescreen Edition)</td>\n",
       "      <td>13.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288400</th>\n",
       "      <td>A1NSHD4YCL5DV3</td>\n",
       "      <td>0792833236</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1063497600</td>\n",
       "      <td>Raging Bull</td>\n",
       "      <td>12.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913721</th>\n",
       "      <td>A2R9J5LULVKF6T</td>\n",
       "      <td>6304176287</td>\n",
       "      <td>4.0</td>\n",
       "      <td>998956800</td>\n",
       "      <td>Willy Wonka &amp;amp; Chocolate Factory [VHS]</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860416</th>\n",
       "      <td>A335GUD1YBS31U</td>\n",
       "      <td>6303617719</td>\n",
       "      <td>5.0</td>\n",
       "      <td>998438400</td>\n",
       "      <td>French Kiss [VHS]</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512736</th>\n",
       "      <td>A1TFH5Y9I9M3YN</td>\n",
       "      <td>B00005QCYC</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1005091200</td>\n",
       "      <td>Jurassic Park Trilogy</td>\n",
       "      <td>36.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewer_id        asin  overall  review_time  \\\n",
       "1393425  A328S9RN3U5M68  B00005JMFQ      5.0   1068681600   \n",
       "288400   A1NSHD4YCL5DV3  0792833236      4.0   1063497600   \n",
       "913721   A2R9J5LULVKF6T  6304176287      4.0    998956800   \n",
       "860416   A335GUD1YBS31U  6303617719      5.0    998438400   \n",
       "1512736  A1TFH5Y9I9M3YN  B00005QCYC      4.0   1005091200   \n",
       "\n",
       "                                             title  price  \n",
       "1393425         Love Actually (Widescreen Edition)  13.70  \n",
       "288400                                 Raging Bull  12.70  \n",
       "913721   Willy Wonka &amp; Chocolate Factory [VHS]   3.99  \n",
       "860416                           French Kiss [VHS]   0.99  \n",
       "1512736                      Jurassic Park Trilogy  36.27  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce size of dataset (REMOVE LATER):\n",
    "# The main issue occurs below during gradient descent.\n",
    "# np.dot() takes a long time. Perhaps I can use PySpark's\n",
    "# dot product. \n",
    "#popular_df = popular_df.head(NUM_OF_REVIEWS)\n",
    "\n",
    "popular_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get ID's to be used as sanity checks\n",
    "person_of_interest = popular_df.iloc[0].reviewer_id\n",
    "product_of_interest = popular_df.iloc[0].asin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The piece of code is a bit of an embarrassment. This will be mentioned again below, but the main reason for this was for convinence. Later, I will be taking the dot product between two matricies. If I use all of the data, this dot product will take a long time. One solution is to take advantage of distributed computing. Perhaps in my next blog post, I will explore this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19118 reviewers\n",
      "There are 1892 products reviewed\n"
     ]
    }
   ],
   "source": [
    "# Only take relevant columns:\n",
    "rating_df = popular_df[['reviewer_id', 'asin', 'overall']]\n",
    "\n",
    "# Obtaining number of reviewers and products reviewed:\n",
    "n_reviewers = len(rating_df.reviewer_id.unique())\n",
    "n_products = len(rating_df.asin.unique())\n",
    "\n",
    "# Print results:\n",
    "print(\"There are\", n_reviewers, \"reviewers\")\n",
    "print(\"There are\", n_products, \"products reviewed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have reviewers and movies. FYI, the reason why I am saying \"product\" in my code is because I was previously using other amazon product data sets available [here](http://jmcauley.ucsd.edu/data/amazon/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are working to construct a matrix where each row is a reviewer and each column is a movie. I initially construct it as a sparse array, which I think is the correct thing to do. Unfortunately, later I simply work with it as a normal dense array. If I were to repeat this exercise, I would have liked to continue to work with it as a sparse matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating map to map from reviewer id/asin(product id) to a number:\n",
    "reviewer_map = dict(zip(np.unique(rating_df.reviewer_id), list(range(n_reviewers))))\n",
    "product_map = dict(zip(np.unique(rating_df.asin), list(range(n_products))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker_df = pd.DataFrame({ 'reviewer_id': np.unique(rating_df.reviewer_id)})\n",
    "\n",
    "person_of_interest_index = np.where(tracker_df.reviewer_id == person_of_interest)[0][0]\n",
    "\n",
    "tracker_df.iloc[person_of_interest_index][0] == person_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtain numbers(index in sparse matrix) associated with each id:\n",
    "reviewer_index = np.array([reviewer_map[reviewer_id] for reviewer_id in rating_df.reviewer_id])\n",
    "product_index = np.array([product_map[asin] for asin in rating_df.asin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtain ratings to be put into sparse matrix:\n",
    "ratings = np.array(rating_df.overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create sparse X matrix:\n",
    "X = coo_matrix((ratings, (reviewer_index, product_index)), shape=(n_reviewers, n_products))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, the sparse matrix `X` has now been constructed. Let's just double check that we did everything right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing sanity check:\n",
    "person_of_interest_index = reviewer_map[person_of_interest]\n",
    "product_of_interest_index = product_map[product_of_interest]\n",
    "\n",
    "X.toarray()[person_of_interest_index, product_of_interest_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, this result makes sense.\n",
    "\n",
    "Now, the rating data is in a sparse matrix where the rows are users and the columns are movies. Note that all values of 0 for ratings are actually missing reviews. Reviewers cannot give something 0 stars. This is because `coo_matrix` by default fills in missing values with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making our first model:\n",
    "\n",
    "Now, let's use gradient descent to make our first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up parameters:\n",
    "n_iters = 1000\n",
    "alpha = 0.01\n",
    "num_latent_features = 2\n",
    "\n",
    "# Using dense array for now:\n",
    "X_array = X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses gradient descent to learn the correct values for `U` and `V`. `U` can be thought as an array where each row is associated with a specific person and each column represents some sort of learned preference of theirs. These learned preferences are the number of latent features. `V` can be thought of as an array where each column is associated with a specific movie and the rows are the learned quality of the movie. This learned quality of the movie corresponds to the learned preference of the movie. \n",
    "\n",
    "For example, let's first assume that one of \"learned preference\" happend to be a preference towards scary movies. The \"learned quality\" of the movie would be how scary the movie was. This is further explained in my last blog post. \n",
    "\n",
    "As an additional note, I am performing gradient descent a little differently than I had in my previous blog post. Essentially, these are both the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomly initialize:\n",
    "# U = np.random.randn(n_reviewers, num_latent_features) * 1e-5\n",
    "# V = np.random.randn(num_latent_features, n_products) * 1e-5\n",
    "\n",
    "# batch_size = 10\n",
    "# n_iters = 1000\n",
    "# # Perform gradient descent:\n",
    "# for i in range(n_iters):\n",
    "    \n",
    "#     # Obtain predictions:\n",
    "#     X_hat = np.dot(U, V)     \n",
    "    \n",
    "#     if np.isnan(X_hat[person_of_interest, product_of_interest]):\n",
    "#         print(\"ERROR\")\n",
    "#         break\n",
    "        \n",
    "#     # Obtain residual\n",
    "#     resid = X_hat - X_array\n",
    "#     resid[np.isnan(resid)] = 0\n",
    "    \n",
    "#     # Calculate gradients:\n",
    "#     dU = np.dot(resid, V.T) \n",
    "#     dV = np.dot(U.T, resid)\n",
    "    \n",
    "#     # Update values:\n",
    "#     U = U - dU*alpha\n",
    "#     V = V - dV*alpha\n",
    "    \n",
    "#     # Output every 10% to make sure on the right track:\n",
    "#     if (i%(n_iters/10) == 0):\n",
    "#         print(\"Iteration:\", i, \n",
    "#               \"   Cost:\", np.sum(resid**2), \n",
    "#               \"   Rating of interest:\", X_hat[person_of_interest, product_of_interest])\n",
    "    \n",
    "# X_pred = np.dot(U, V)\n",
    "\n",
    "# predicted_val = X_pred[person_of_interest, product_of_interest]\n",
    "# actual_val = X_array[person_of_interest, product_of_interest]\n",
    "# print(\"Actual Rating: \", actual_val)\n",
    "# print(\"Predicted Rating: \", predicted_val)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[person_of_interest_index, product_of_interest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_array, tracker_df) = shuffle(X_array, tracker_df)\n",
    "\n",
    "person_of_interest_index = np.where(tracker_df.reviewer_id == person_of_interest)[0][0]\n",
    "\n",
    "X_array[person_of_interest_index, product_of_interest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0    Cost: 0.22821\n",
      "Iteration: 10    Cost: 0.229306999992\n",
      "Iteration: 20    Cost: 0.247088999705\n",
      "Iteration: 30    Cost: 0.228804987128\n",
      "Iteration: 40    Cost: 0.257570743194\n",
      "Iteration: 50    Cost: 0.260380797897\n",
      "Iteration: 60    Cost: 0.231674184023\n",
      "Iteration: 70    Cost: 0.158370155337\n",
      "Iteration: 80    Cost: 0.0862964856584\n",
      "Iteration: 90    Cost: 0.050658998503\n",
      "Iteration: 100    Cost: 0.0488122168182\n",
      "Iteration: 110    Cost: 0.0278062611394\n",
      "Iteration: 120    Cost: 0.0212366496089\n",
      "Iteration: 130    Cost: 0.0445085836018\n",
      "Iteration: 140    Cost: 0.0171101456195\n",
      "Iteration: 150    Cost: 0.0137202750717\n",
      "Iteration: 160    Cost: 0.0141128796891\n",
      "Iteration: 170    Cost: 0.0141720350633\n",
      "Iteration: 180    Cost: 0.0131744315898\n",
      "Iteration: 190    Cost: 0.012571531832\n",
      "Iteration: 200    Cost: 0.0156136661542\n",
      "Iteration: 210    Cost: 0.0124520643627\n",
      "Iteration: 220    Cost: 0.011726567046\n",
      "Iteration: 230    Cost: 0.0234159807979\n",
      "Iteration: 240    Cost: 0.0123934069533\n",
      "Iteration: 250    Cost: 0.011677762054\n",
      "Iteration: 260    Cost: 0.0116589714614\n",
      "Iteration: 270    Cost: 0.0125683717034\n",
      "Iteration: 280    Cost: 0.0104371771465\n",
      "Iteration: 290    Cost: 0.0111810682538\n",
      "Iteration: 300    Cost: 0.012143014458\n",
      "Iteration: 310    Cost: 0.0106624585965\n",
      "Iteration: 320    Cost: 0.0100219794834\n",
      "Iteration: 330    Cost: 0.0111551826307\n",
      "Iteration: 340    Cost: 0.0116387771991\n",
      "Iteration: 350    Cost: 0.0652684539205\n",
      "Iteration: 360    Cost: 0.0115543002947\n",
      "Iteration: 370    Cost: 0.0111290281113\n",
      "Iteration: 380    Cost: 0.0113088351889\n",
      "Iteration: 390    Cost: 0.0108761191933\n",
      "Iteration: 400    Cost: 0.0109267856986\n",
      "Iteration: 410    Cost: 0.0112526113587\n",
      "Iteration: 420    Cost: 0.0101234029648\n",
      "Iteration: 430    Cost: 0.00965828821973\n",
      "Iteration: 440    Cost: 0.0108053120317\n",
      "Iteration: 450    Cost: 0.0111993333941\n",
      "Iteration: 460    Cost: 0.0377506758708\n",
      "Iteration: 470    Cost: 0.011631298608\n",
      "Iteration: 480    Cost: 0.00966248545679\n",
      "Iteration: 490    Cost: 0.0111799042648\n",
      "Actual Rating:  5.0\n",
      "Predicted Rating:  4.18402082033\n"
     ]
    }
   ],
   "source": [
    "# Replacing zeros with nan:\n",
    "X_array[X_array==0] = np.nan\n",
    "\n",
    "# Randomly initialize:\n",
    "num_latent_features = 10\n",
    "U = np.random.randn(n_reviewers, num_latent_features) * 1e-5\n",
    "V = np.random.randn(num_latent_features, n_products) * 1e-5\n",
    "k = 1\n",
    "\n",
    "X_hat = U@V\n",
    "\n",
    "batch_size = 1000\n",
    "n_iters = 500\n",
    "\n",
    "alpha = 0.0005\n",
    "\n",
    "# Perform gradient descent:\n",
    "for i in range(n_iters):\n",
    "    \n",
    "    # Shuffle users:\n",
    "    (X_array, U, tracker_df) = shuffle(X_array, U, tracker_df)\n",
    "    person_of_interest_index = np.where(tracker_df.reviewer_id == person_of_interest)[0][0]\n",
    "    \n",
    "    for j in range(0, n_reviewers, batch_size):\n",
    "        # Make sure doesn't pass array size\n",
    "        if j + batch_size > n_reviewers:\n",
    "            break\n",
    "\n",
    "        # Select mini batches of data:\n",
    "        mini_X_array = X_array[j:j+batch_size, :]\n",
    "        mini_U = U[j:j+batch_size, :]\n",
    "\n",
    "        # Calculate predicted values\n",
    "        mini_X_hat = mini_U@V\n",
    "        #X_hat[j:j+batch_size, :] = mini_X_hat\n",
    "\n",
    "        # Calculate error\n",
    "        resid = mini_X_hat - mini_X_array\n",
    "        resid[np.isnan(resid)] = 0 \n",
    "\n",
    "        mini_dU = np.dot(resid, V.T)\n",
    "        dV = np.dot(mini_U.T, resid)\n",
    "\n",
    "        U[j:j+batch_size] = U[j:j+batch_size] - mini_dU*alpha\n",
    "        V = V - dV*alpha\n",
    "        \n",
    "    if i%10 == 0:\n",
    "        print( \"Iteration:\", i,  \n",
    "               \"   Cost:\", np.sum((resid/batch_size)**2))\n",
    "\n",
    "    \n",
    "X_pred = U@V\n",
    "predicted_val = X_pred[person_of_interest_index, product_of_interest_index]\n",
    "actual_val = X_array[person_of_interest_index, product_of_interest_index]\n",
    "\n",
    "print(\"Actual Rating: \", actual_val)\n",
    "print(\"Predicted Rating: \", predicted_val)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1819804483941557"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rating_df\n",
    "\n",
    "# val1 = np.where(tracker_df.reviewer_id == \"A2O6TVTQEEWS4E\")[0][0]\n",
    "# val2 = product_map['0310274281']\n",
    "\n",
    "# X_pred[val1, val2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above prediction is awful. This is because we didn't handle `nan` properly. The algorithm above is trying to account for all the zero values. The \"signal\" is being drowned out. To fix this is simple:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making our second model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewlim/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1f1fd9a8f694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mX_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperson_of_interest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_of_interest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Replacing zeros with nan:\n",
    "X_array[X_array==0] = np.nan\n",
    "\n",
    "# Set up parameters:\n",
    "n_iters = 1000\n",
    "alpha = 0.01\n",
    "num_latent_features = 2\n",
    "\n",
    "# Initialize U and V randomly\n",
    "U = np.random.randn(n_reviewers, num_latent_features) * 1e-5\n",
    "V = np.random.randn(num_latent_features, n_products) * 1e-5\n",
    "\n",
    "# Perform gradient descent:\n",
    "for i in range(n_iters):        \n",
    "    # Obtain predictions:\n",
    "    X_hat = np.dot(U, V)     \n",
    "    \n",
    "    if np.isnan(X_hat[person_of_interest, product_of_interest]):\n",
    "        print(\"ERROR\")\n",
    "        break\n",
    "        \n",
    "    # Obtain residual\n",
    "    resid = X_hat - X_array\n",
    "    resid[np.isnan(resid)] = 0\n",
    "    \n",
    "    # Calculate gradients:\n",
    "    dU = np.dot(resid, V.T) \n",
    "    dV = np.dot(U.T, resid)\n",
    "    \n",
    "    # Update values:\n",
    "    U = U - dU*alpha\n",
    "    V = V - dV*alpha\n",
    "    \n",
    "    # Output every 10% to make sure on the right track:\n",
    "    if (i%(n_iters/10) == 0):\n",
    "        print(\"Iteration:\", i, \n",
    "              \"   Cost:\", np.sum(resid**2), \n",
    "              \"   Rating of interest:\", X_hat[person_of_interest, product_of_interest])\n",
    "    \n",
    "# Make prediction:\n",
    "X_pred = np.dot(U, V)\n",
    "\n",
    "predicted_val = X_pred[person_of_interest, product_of_interest]\n",
    "actual_val = X_array[person_of_interest, product_of_interest]\n",
    "print(\"Actual Rating: \", actual_val)\n",
    "print(\"Predicted Rating: \", predicted_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a huge improvement! Awesome! Next, I will try to do some analysis on these results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "\n",
    "One idea that can be done as a result from our model is we can group movies together. We have obtained these \"learned features\", why not use them? So now, I will try to make a plot to visualize the learned features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try to see which products are most similar to each other:\n",
    "param1 = V.T[:,0]\n",
    "param2 = V.T[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting the products:\n",
    "plt.plot(param1, param2, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, when using 2 latent-features we seem to have movies that form a circular shape! I am not sure why, but it looks interesting! Now, let's try to cluster them together to see which movies are most similar to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Performing k-means:\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(V.T)\n",
    "groups = kmeans.labels_\n",
    "\n",
    "# Making plot:\n",
    "plt.scatter(param1, param2, c=groups)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a sprinkled donut. It would be nice to see concrete movie titles associated with each of these groups. This is done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making dictionary connecting asin to with title and group:\n",
    "asin = np.unique(popular_df.asin)\n",
    "product_dictionary = dict(zip(popular_df.asin, popular_df.title))\n",
    "product_results = pd.DataFrame({'asin': asin})\n",
    "product_results['title'] = [product_dictionary[x] for x in product_results.asin]\n",
    "product_results['group'] = groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Showing group 0:\n",
    "product_results[product_results.group == 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe this group is associated with movies that are action-y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Showing group 1:\n",
    "product_results[product_results.group == 1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure what this group is exactly, but I'm happy that it placed the two Upstairs Downstairs titles in the same group!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Showing group 2:\n",
    "product_results[product_results.group == 2].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really have no idea as to what is occurring in this group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Notes:\n",
    "\n",
    "Unfortunately, these groups don't seem to make immediate sense. Honestly, the trends within groups that seem to be present are probably by luck. It would be nice to see groups of movies that had an obvious theme going on such as genre or time when released. Perhaps in the future, we can make better groups if we use more latent features (we used 2 in this case). \n",
    "\n",
    "In future blog posts I plan to:\n",
    "* Scale up to use more data\n",
    "* Create a twitter bot that may be able to identify which movies are trending on social media\n",
    "    * This can be used to promote movies that are trending to people that we predicted would give it a high rating. \n",
    "    * The same idea can be applied to Amazon's \"Toys and Games\" data set. My thought that it would be able to promote something like a fidget spinner to people which seems to be all the rage now a days. \n",
    "* Create better models be adding more features to our dataset. This can easily be done by altering our gradient descent loop. \n",
    "\n",
    "Please let me know if you have any other cool ideas/questions/suggestions/**found mistakes**! My e-mail is andrewlim90@gmail.com"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
